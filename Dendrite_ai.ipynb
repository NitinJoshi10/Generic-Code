{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "fyN45dYw41Zm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, StratifiedKFold, KFold\n",
        "from sklearn.metrics import auc, f1_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, RobustScaler\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "\n",
        "# Load the JSON configuration\n",
        "with open(\"/content/json-fixer.json\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "\n",
        "def extract_session_info(config):\n",
        "    session_info = config['design_state_data']['session_info']\n",
        "    project_id = session_info['project_id']\n",
        "    experiment_id = session_info['experiment_id']\n",
        "    dataset = session_info['dataset']\n",
        "    session_name = session_info['session_name']\n",
        "    session_description = session_info['session_description']\n",
        "\n",
        "    return project_id, experiment_id, dataset, session_name, session_description\n",
        "\n",
        "\n",
        "\n",
        "# data is the dataset containing the features and target variable\n",
        "#data = pd.read_csv('/content/iris.csv')\n",
        "# You can replace 'data' with your actual dataset\n",
        "\n",
        "\n",
        "#target\n",
        "prediction_type = config[\"design_state_data\" ]['target']['prediction_type'],\n",
        "target_column = config[\"design_state_data\" ]['target']['target'],\n",
        "X = data.drop(target_column, axis=1)\n",
        "y = data[target_column]\n",
        "partitioning = config[\"design_state_data\" ]['target']['partitioning']\n",
        "\n",
        "\n",
        "#train policy\n",
        "\n",
        "train_policy = config['train']['policy'],\n",
        "variable = config['train']['time_variable'],\n",
        "sampling_method = config['train']['sampling_method'],\n",
        "split_method = config['train']['split']\n",
        "k_fold = config['train']['k_fold']\n",
        "test_size=config['train']['train_ratio'],\n",
        "random_state=config['train']['random_seed'],\n",
        "\n",
        "\n",
        "# Train-test split\n",
        "#if config['train']['split'] == 'Randomly':\n",
        "#    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config['train']['train_ratio'], random_state=config['train']['random_seed'])\n",
        "#else:\n",
        "    # Implement other partitioning strategies if needed\n",
        "#    pass\n",
        "\n",
        "#Evaluation metrices\n",
        "\n",
        "def optimize_metrics(config, y_true, y_pred):\n",
        "    # Extract metrics configuration details\n",
        "    opt_metric = config['optomize_model_hyperparameters_for']\n",
        "    opt_threshold_metric = config['optimize_threshold_for']\n",
        "    compute_lift_at = config['compute_lift_at']\n",
        "\n",
        "    # Compute the specified metrics\n",
        "    if opt_metric == 'AUC':\n",
        "        # Implement AUC calculation based on y_true and y_pred\n",
        "        # auc_score = calculate_auc(y_true, y_pred)\n",
        "        pass\n",
        "    elif opt_metric == 'Accuracy':\n",
        "        # Implement accuracy calculation based on y_true and y_pred\n",
        "        # accuracy = calculate_accuracy(y_true, y_pred)\n",
        "        pass\n",
        "    # Add more metrics as needed\n",
        "\n",
        "    if opt_threshold_metric == 'F1 Score':\n",
        "        # Optimize the threshold for F1 Score and update y_pred accordingly\n",
        "        # f1_optimized_y_pred = optimize_f1_score_threshold(y_true, y_pred)\n",
        "        # y_pred = f1_optimized_y_pred\n",
        "        pass\n",
        "    elif opt_threshold_metric == 'Other Metric':\n",
        "        # Implement other threshold optimization based on the chosen metric\n",
        "        pass\n",
        "    # Add more threshold optimization methods as needed\n",
        "\n",
        "    if compute_lift_at > 0:\n",
        "        # Compute lift at the specified percentile (compute_lift_at)\n",
        "        # lift_score = compute_lift(y_true, y_pred, compute_lift_at)\n",
        "        pass\n",
        "    # Add more lift computation or other metric-specific calculations as needed\n",
        "\n",
        "    # Compute cost matrix or other metric-specific evaluations based on the gains\n",
        "    cost_matrix_gain_for_true_pred_true_result = config['cost_matrix_gain_for_true_prediction_true_result']\n",
        "    cost_matrix_gain_for_true_pred_false_result = config['cost_matrix_gain_for_true_prediction_false_result']\n",
        "    cost_matrix_gain_for_false_pred_true_result = config['cost_matrix_gain_for_false_prediction_true_result']\n",
        "    cost_matrix_gain_for_false_pred_false_result = config['cost_matrix_gain_for_false_prediction_false_result']\n",
        "\n",
        "    # Implement the cost matrix calculation or other metric-specific evaluations based on the gains\n",
        "    # cost_matrix = calculate_cost_matrix(y_true, y_pred, cost_matrix_gain_for_true_pred_true_result, ...)\n",
        "\n",
        "    # Return the computed metrics or any other relevant information\n",
        "    # return auc_score, accuracy, lift_score, cost_matrix\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'config' is the configuration dictionary for metrics as mentioned above\n",
        "# Let's say 'y_true' is the true labels and 'y_pred' is the predicted labels from the model\n",
        "# You can compute the metrics using the function like this:\n",
        "\n",
        "# metrics_results = optimize_metrics(config, y_true, y_pred)\n",
        "# Now, the 'metrics_results' will contain the computed metrics or any other relevant information.\n",
        "\n",
        "\n",
        "\n",
        "#feature handling\n",
        "\n",
        "def handle_feature(config, data):\n",
        "    # Extract feature name and details from the configuration\n",
        "    feature_name = config['feature_name']\n",
        "    is_selected = config['is_selected']\n",
        "    feature_variable_type = config['feature_variable_type']\n",
        "\n",
        "    if not is_selected:\n",
        "        # If the feature is not selected, simply drop it from the data\n",
        "        data.drop(feature_name, axis=1, inplace=True)\n",
        "        return data\n",
        "\n",
        "    if feature_variable_type == 'numerical':\n",
        "        # Numerical feature handling\n",
        "        numerical_handling = config['feature_details']['numerical_handling']\n",
        "\n",
        "        if numerical_handling == 'Keep as regular numerical feature':\n",
        "            # No special handling required\n",
        "            pass\n",
        "        elif numerical_handling == 'Rescale':\n",
        "            # Perform rescaling (e.g., Min-Max scaling, Standardization)\n",
        "            # Implement rescaling here based on the chosen method\n",
        "            pass\n",
        "\n",
        "        # Handle missing values\n",
        "        missing_values = config['feature_details']['missing_values']\n",
        "        if missing_values == 'Impute':\n",
        "            impute_with = config['feature_details']['impute_with']\n",
        "            impute_value = config['feature_details']['impute_value']\n",
        "            if impute_with == 'Average of values':\n",
        "                # Impute missing values with the average of non-missing values\n",
        "                data[feature_name].fillna(data[feature_name].mean(), inplace=True)\n",
        "            elif impute_with == 'custom':\n",
        "                # Impute missing values with a custom value\n",
        "                data[feature_name].fillna(impute_value, inplace=True)\n",
        "\n",
        "    elif feature_variable_type == 'text':\n",
        "        # Text feature handling\n",
        "        text_handling = config['feature_details']['text_handling']\n",
        "        if text_handling == 'Tokenize and hash':\n",
        "            # Tokenize the text and apply hashing (e.g., feature hashing)\n",
        "            # Implement tokenization and hashing here based on the chosen method\n",
        "            pass\n",
        "\n",
        "    # If 'make_derived_feats' is True, create derived features based on the original feature.\n",
        "    make_derived_feats = config['feature_details']['make_derived_feats']\n",
        "    if make_derived_feats:\n",
        "        # Implement derived feature creation based on the original feature\n",
        "        pass\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'data' is the dataset containing the features mentioned in the configuration\n",
        "# Let's say 'config' is the configuration dictionary for feature handling as mentioned above\n",
        "# You can handle each feature using the function like this:\n",
        "\n",
        "for feature_name, config in config.items():\n",
        "    data = handle_feature(config, data)\n",
        "\n",
        "# Now, the 'data' will be modified according to the feature handling operations specified in the configuration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Feature generation\n",
        "\n",
        "def generate_features(config, data):\n",
        "    # Extract feature generation configuration details\n",
        "    linear_interactions = config['linear_interactions']\n",
        "    linear_scalar_type = config['linear_scalar_type']\n",
        "    polynomial_interactions = config['polynomial_interactions']\n",
        "    explicit_pairwise_interactions = config['explicit_pairwise_interactions']\n",
        "\n",
        "    # Perform linear interactions\n",
        "    for interaction in linear_interactions:\n",
        "        feature_name = f\"{interaction[0]}*{interaction[1]}\"\n",
        "        data[feature_name] = data[interaction[0]] * data[interaction[1]]\n",
        "\n",
        "    # Perform polynomial interactions\n",
        "    for interaction in polynomial_interactions:\n",
        "        interaction_features = interaction.split('/')\n",
        "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "        poly_features = poly.fit_transform(data[interaction_features])\n",
        "        poly_feature_names = poly.get_feature_names(interaction_features)\n",
        "        poly_feature_df = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
        "        data = pd.concat([data, poly_feature_df], axis=1)\n",
        "\n",
        "    # Perform explicit pairwise interactions\n",
        "    for interaction in explicit_pairwise_interactions:\n",
        "        interaction_features = interaction.split('/')\n",
        "        feature_name = f\"{interaction_features[0]}_{interaction_features[1]}\"\n",
        "        data[feature_name] = data[interaction_features[0]] + data[interaction_features[1]]\n",
        "\n",
        "    # Perform linear scaling if required (e.g., RobustScaler)\n",
        "    if linear_scalar_type == 'robust':\n",
        "        scaler = RobustScaler()\n",
        "        scaled_features = scaler.fit_transform(data)\n",
        "        data = pd.DataFrame(scaled_features, columns=data.columns)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'data' is the dataset containing the original features as mentioned in the configuration\n",
        "# Let's say 'config' is the configuration dictionary for feature generation as mentioned above\n",
        "# You can generate the new features using the function like this:\n",
        "\n",
        "# data = generate_features(config['feature_generation'], data)\n",
        "# Now, the 'data' will contain the original features along with the generated linear, polynomial, and explicit pairwise interactions.\n",
        "# Additionally, if specified, the data will be linearly scaled using the RobustScaler.\n",
        "\n",
        "#feature reduction\n",
        "\n",
        "def perform_feature_reduction(config, X_train, y_train):\n",
        "    # Extract feature reduction configuration details\n",
        "    feature_reduction_method = config['feature_reduction_method']\n",
        "    num_of_features_to_keep = int(config['num_of_features_to_keep'])\n",
        "    num_of_trees = int(config['num_of_trees'])\n",
        "    depth_of_trees = int(config['depth_of_trees'])\n",
        "\n",
        "    # Initialize the feature reduction model\n",
        "    if feature_reduction_method == 'Tree-based':\n",
        "        model = RandomForestClassifier(n_estimators=num_of_trees, max_depth=depth_of_trees)\n",
        "    elif feature_reduction_method == 'Tree-based Regression':\n",
        "        model = RandomForestRegressor(n_estimators=num_of_trees, max_depth=depth_of_trees)\n",
        "    else:\n",
        "        # Implement other feature reduction methods if needed\n",
        "        return X_train\n",
        "\n",
        "#hyperparameters\n",
        "\n",
        "def optimize_hyperparameters(config, model, X_train, y_train):\n",
        "    # Extract hyperparameter configuration details\n",
        "    hyperparameter_strategy = config['stratergy']\n",
        "    shuffle_grid = config['shuffle_grid']\n",
        "    random_state = config['random_state']\n",
        "    max_iterations = config['max_iterations']\n",
        "    max_search_time = config['max_search_time']\n",
        "    parallelism = config['parallelism']\n",
        "    cross_validation_strategy = config['cross_validation_stratergy']\n",
        "    num_of_folds = config['num_of_folds']\n",
        "    split_ratio = config['split_ratio']\n",
        "    stratified = config['stratified']\n",
        "\n",
        "    # Create a dictionary of hyperparameters for the grid search\n",
        "    hyperparameter_grid = {\n",
        "        # Add hyperparameter options for grid search\n",
        "        # Example: 'param_name': [value1, value2, ...]\n",
        "    }\n",
        "\n",
        "    # Create a cross-validation strategy based on the specified configuration\n",
        "    if cross_validation_strategy == 'Time-based K-fold(with overlap)':\n",
        "        cv = TimeSeriesSplit(n_splits=num_of_folds)\n",
        "    elif cross_validation_strategy == 'Stratified K-fold':\n",
        "        if stratified:\n",
        "            cv = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=random_state)\n",
        "        else:\n",
        "            cv = KFold(n_splits=num_of_folds, shuffle=True, random_state=random_state)\n",
        "    else:\n",
        "        # Implement other cross-validation strategies if needed\n",
        "        cv = None\n",
        "\n",
        "    # Initialize the GridSearchCV with the model and hyperparameter grid\n",
        "    grid_search = GridSearchCV(model, hyperparameter_grid, cv=cv, n_jobs=parallelism, verbose=2)\n",
        "\n",
        "    # Perform grid search to find the best hyperparameters\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model with optimized hyperparameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'model' is the base model, 'X_train' is the training feature data, and 'y_train' is the corresponding target data\n",
        "# Let's say 'config' is the configuration dictionary for hyperparameters as mentioned above\n",
        "# You can perform hyperparameter optimization using the function like this:\n",
        "\n",
        "# best_model = optimize_hyperparameters(config['hyperparameters'], model, X_train, y_train)\n",
        "# Now, the 'best_model' will be the model with the optimized hyperparameters based on the grid search.\n",
        "\n",
        "\n",
        "# Weight stratagy\n",
        "\n",
        "def apply_weighting_strategy(config, data):\n",
        "    # Extract weighting strategy configuration details\n",
        "    weighting_strategy_method = config['weighting_stratergy_method']\n",
        "    weighting_strategy_weight_variable = config['weighting_stratergy_weight_variable']\n",
        "\n",
        "    if weighting_strategy_method == 'Sample weights':\n",
        "        # Implement sample weighting strategy based on the specified weight variable\n",
        "        # Calculate the weights based on the 'weighting_strategy_weight_variable'\n",
        "        # For example, if 'petal_length' is the weight variable, you can calculate the weights as follows:\n",
        "        # data['weights'] = calculate_weights_based_on_petal_length(data[weighting_strategy_weight_variable])\n",
        "\n",
        "        # Ensure that the 'weights' column is available in the dataset before using it in the model\n",
        "        # For example, you may have to modify your model training code to include the 'weights' parameter:\n",
        "        # model.fit(X_train, y_train, sample_weight=data['weights'])\n",
        "        pass\n",
        "    elif weighting_strategy_method == 'Other Weighting Method':\n",
        "        # Implement other weighting strategies based on the chosen method\n",
        "        pass\n",
        "    # Add more weighting strategies as needed\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'data' is the dataset containing the features and target variable as mentioned in the configuration\n",
        "# Let's say 'config' is the configuration dictionary for the weighting strategy as mentioned above\n",
        "# You can apply the weighting strategy using the function like this:\n",
        "\n",
        "# data = apply_weighting_strategy(config['weighting_stratergy'], data)\n",
        "# Now, the 'data' will contain the weights based on the specified weighting strategy.\n",
        "# Make sure to use these weights appropriately in your model training code.\n",
        "\n",
        "# Probability calibration\n",
        "\n",
        "\n",
        "def perform_probability_calibration(config, model, X_train, y_train):\n",
        "    # Extract probability calibration configuration details\n",
        "    calibration_method = config['probability_calibration_method']\n",
        "\n",
        "    # Initialize the CalibratedClassifierCV with the model and calibration method\n",
        "    if calibration_method == 'Sigmoid - Platt Scaling':\n",
        "        calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
        "    elif calibration_method == 'Isotonic':\n",
        "        calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
        "    else:\n",
        "        # Implement other probability calibration methods if needed\n",
        "        return model\n",
        "\n",
        "    # Fit the calibrated model on the training data\n",
        "    calibrated_model.fit(X_train, y_train)\n",
        "\n",
        "    return calibrated_model\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'model' is the base model, 'X_train' is the training feature data, and 'y_train' is the corresponding target data\n",
        "# Let's say 'config' is the configuration dictionary for probability calibration as mentioned above\n",
        "# You can perform probability calibration using the function like this:\n",
        "\n",
        "# calibrated_model = perform_probability_calibration(config['probability_calibration'], model, X_train, y_train)\n",
        "# Now, the 'calibrated_model' will be the model with probability calibration based on the chosen method.\n",
        "\n",
        "# Model\n",
        "\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(config, X_train, y_train, X_test, y_test):\n",
        "    model_name = config['model_name']\n",
        "    if not config['is_selected']:\n",
        "        print(f\"Skipping {model_name} as it is not selected.\")\n",
        "        return\n",
        "\n",
        "    if model_name == 'Random Forest Classifier':\n",
        "        model = RandomForestClassifier(n_estimators=config['max_trees'],\n",
        "                                       min_samples_leaf=config['min_samples_per_leaf_min_value'],\n",
        "                                       max_depth=config['max_depth'],\n",
        "                                       random_state=0)\n",
        "    elif model_name == 'Random Forest Regressor':\n",
        "        model = RandomForestRegressor(n_estimators=config['max_trees'],\n",
        "                                      min_samples_leaf=config['min_samples_per_leaf_min_value'],\n",
        "                                      max_depth=config['max_depth'],\n",
        "                                      random_state=0)\n",
        "    elif model_name == 'Gradient Boosted Trees':\n",
        "        model = GradientBoostingClassifier(n_estimators=config['num_of_BoostingStages'][0],\n",
        "                                           max_depth=config['max_depth'])\n",
        "    elif model_name == 'Gradient Boosted Trees Regressor':\n",
        "        model = GradientBoostingRegressor(n_estimators=config['num_of_BoostingStages'][0],\n",
        "                                          max_depth=config['max_depth'])\n",
        "    elif model_name == 'LinearRegression':\n",
        "        model = LinearRegression()\n",
        "    elif model_name == 'LogisticRegression':\n",
        "        model = LogisticRegression()\n",
        "    elif model_name == 'RidgeRegression':\n",
        "        model = Ridge()\n",
        "    elif model_name == 'Lasso Regression':\n",
        "        model = Lasso()\n",
        "    elif model_name == 'ElasticNetRegression':\n",
        "        model = ElasticNet()\n",
        "    elif model_name == 'XG Boost':\n",
        "        model = XGBClassifier(n_estimators=config['max_num_of_trees'],\n",
        "                              max_depth=config['max_depth_of_tree'][0],\n",
        "                              learning_rate=config['learningRate'][0],\n",
        "                              reg_alpha=config['l1_regularization'][0],\n",
        "                              reg_lambda=config['l2_regularization'][0],\n",
        "                              gamma=config['gamma'][0],\n",
        "                              subsample=config['sub_sample'][0],\n",
        "                              colsample_bytree=config['col_sample_by_tree'][0],\n",
        "                              random_state=0)\n",
        "    elif model_name == 'Decision Tree':\n",
        "        model = DecisionTreeClassifier(min_samples_leaf=config['min_samples_per_leaf'][0],\n",
        "                                       max_depth=config['max_depth'],\n",
        "                                       criterion='entropy' if config['use_entropy'] else 'gini')\n",
        "    elif model_name == 'Support Vector Machine':\n",
        "        model = SVC(C=config['c_value'][0], kernel='linear')\n",
        "    elif model_name == 'Stochastic Gradient Descent':\n",
        "        model = SGDClassifier(loss='log' if config['use_logistics'] else 'modified_huber',\n",
        "                              alpha=config['alpha_value'][0],\n",
        "                              penalty='l1' if config['use_l1_regularization'] else 'l2',\n",
        "                              l1_ratio=config['use_elastic_net_regularization'],\n",
        "                              random_state=config['random_state'])\n",
        "    elif model_name == 'KNN':\n",
        "        model = KNeighborsClassifier(n_neighbors=config['k_value'][0], weights='distance' if config['distance_weighting'] else 'uniform')\n",
        "    elif model_name == 'Extra Random Trees':\n",
        "        model = ExtraTreesRegressor(n_estimators=config['num_of_trees'][0],\n",
        "                                    max_depth=config['max_depth'][0],\n",
        "                                    min_samples_leaf=config['min_samples_per_leaf'][0])\n",
        "    elif model_name == 'Neural Network':\n",
        "        model = MLPClassifier(hidden_layer_sizes=config['hidden_layer_sizes'],\n",
        "                              activation=config['activation'],\n",
        "                              alpha=config['alpha_value'],\n",
        "                              max_iter=config['max_iterations'],\n",
        "                              tol=config['convergence_tolerance'],\n",
        "                              solver=config['solver'],\n",
        "                              shuffle=config['shuffle_data'],\n",
        "                              learning_rate_init=config['initial_learning_rate'],\n",
        "                              batch_size='auto' if config['automatic_batching'] else None,\n",
        "                              beta_1=config['beta_1'],\n",
        "                              beta_2=config['beta_2'],\n",
        "                              epsilon=config['epsilon'],\n",
        "                              power_t=config['power_t'],\n",
        "                              momentum=config['momentum'],\n",
        "                              nesterovs_momentum=config['use_nesterov_momentum'])\n",
        "    else:\n",
        "        print(f\"Unknown model name: {model_name}\")\n",
        "        return\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Add evaluation metrics based on the problem type (classification/regression) if needed\n",
        "    if config['target']['type'] == 'classification':\n",
        "       accuracy = accuracy_score(y_test, y_pred)\n",
        "       print(f\"{model_name} - Accuracy:\", accuracy)\n",
        "\n",
        "    elif config['target']['type'] == 'regression':\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{model_name} - Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = open(\"/content/json-fixer.json\", 'r', encoding='utf-8')\n",
        "config_data = json.load(config)\n",
        "config_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRrTfE84beNd",
        "outputId": "a914f03d-54b5-4adf-9d05-c2439d1ac9fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'session_name': 'test',\n",
              " 'session_description': 'test',\n",
              " 'design_state_data': {'session_info': {'project_id': '1',\n",
              "   'experiment_id': 'kkkk-11',\n",
              "   'dataset': 'iris_modified.csv',\n",
              "   'session_name': 'test',\n",
              "   'session_description': 'test'},\n",
              "  'target': {'prediction_type': 'Regression',\n",
              "   'target': 'petal_width',\n",
              "   'type': 'regression',\n",
              "   'partitioning': True},\n",
              "  'train': {'policy': 'Split the dataset',\n",
              "   'time_variable': 'sepal_length',\n",
              "   'sampling_method': 'No sampling(whole data)',\n",
              "   'split': 'Randomly',\n",
              "   'k_fold': False,\n",
              "   'train_ratio': 0,\n",
              "   'random_seed': 0},\n",
              "  'metrics': {'optomize_model_hyperparameters_for': 'AUC',\n",
              "   'optimize_threshold_for': 'F1 Score',\n",
              "   'compute_lift_at': 0,\n",
              "   'cost_matrix_gain_for_true_prediction_true_result': 1,\n",
              "   'cost_matrix_gain_for_true_prediction_false_result': 0,\n",
              "   'cost_matrix_gain_for_false_prediction_true_result': 0,\n",
              "   'cost_matrix_gain_for_false_prediction_false_result': 0},\n",
              "  'feature_handling': {'sepal_length': {'feature_name': 'sepal_length',\n",
              "    'is_selected': True,\n",
              "    'feature_variable_type': 'numerical',\n",
              "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
              "     'rescaling': 'No rescaling',\n",
              "     'make_derived_feats': False,\n",
              "     'missing_values': 'Impute',\n",
              "     'impute_with': 'Average of values',\n",
              "     'impute_value': 0}},\n",
              "   'sepal_width': {'feature_name': 'sepal_width',\n",
              "    'is_selected': True,\n",
              "    'feature_variable_type': 'numerical',\n",
              "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
              "     'rescaling': 'No rescaling',\n",
              "     'make_derived_feats': False,\n",
              "     'missing_values': 'Impute',\n",
              "     'impute_with': 'custom',\n",
              "     'impute_value': -1}},\n",
              "   'petal_length': {'feature_name': 'petal_length',\n",
              "    'is_selected': True,\n",
              "    'feature_variable_type': 'numerical',\n",
              "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
              "     'rescaling': 'No rescaling',\n",
              "     'make_derived_feats': False,\n",
              "     'missing_values': 'Impute',\n",
              "     'impute_with': 'Average of values',\n",
              "     'impute_value': 0}},\n",
              "   'petal_width': {'feature_name': 'petal_width',\n",
              "    'is_selected': True,\n",
              "    'feature_variable_type': 'numerical',\n",
              "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
              "     'rescaling': 'No rescaling',\n",
              "     'make_derived_feats': False,\n",
              "     'missing_values': 'Impute',\n",
              "     'impute_with': 'custom',\n",
              "     'impute_value': -2}},\n",
              "   'species': {'feature_name': 'species',\n",
              "    'is_selected': True,\n",
              "    'feature_variable_type': 'text',\n",
              "    'feature_details': {'text_handling': 'Tokenize and hash',\n",
              "     'hash_columns': 0}}},\n",
              "  'feature_generation': {'linear_interactions': [['petal_length',\n",
              "     'sepal_width']],\n",
              "   'linear_scalar_type': 'robust',\n",
              "   'polynomial_interactions': ['petal_length/sepal_width',\n",
              "    'petal_width/species'],\n",
              "   'explicit_pairwise_interactions': ['sepal_width/sepal_length',\n",
              "    'petal_width/sepal_length']},\n",
              "  'feature_reduction': {'feature_reduction_method': 'Tree-based',\n",
              "   'num_of_features_to_keep': '4',\n",
              "   'num_of_trees': '5',\n",
              "   'depth_of_trees': '6'},\n",
              "  'hyperparameters': {'stratergy': 'Grid Search',\n",
              "   'shuffle_grid': True,\n",
              "   'random_state': 1,\n",
              "   'max_iterations': 2,\n",
              "   'max_search_time': 3,\n",
              "   'parallelism': 5,\n",
              "   'cross_validation_stratergy': 'Time-based K-fold(with overlap)',\n",
              "   'num_of_folds': 6,\n",
              "   'split_ratio': 0,\n",
              "   'stratified': True},\n",
              "  'weighting_stratergy': {'weighting_stratergy_method': 'Sample weights',\n",
              "   'weighting_stratergy_weight_variable': 'petal_length'},\n",
              "  'probability_calibration': {'probability_calibration_method': 'Sigmoid - Platt Scaling'},\n",
              "  'algorithms': {'RandomForestClassifier': {'model_name': 'Random Forest Classifier',\n",
              "    'is_selected': False,\n",
              "    'min_trees': 10,\n",
              "    'max_trees': 30,\n",
              "    'feature_sampling_statergy': 'Default',\n",
              "    'min_depth': 20,\n",
              "    'max_depth': 30,\n",
              "    'min_samples_per_leaf_min_value': 5,\n",
              "    'min_samples_per_leaf_max_value': 50,\n",
              "    'parallelism': 0},\n",
              "   'RandomForestRegressor': {'model_name': 'Random Forest Regressor',\n",
              "    'is_selected': True,\n",
              "    'min_trees': 10,\n",
              "    'max_trees': 20,\n",
              "    'feature_sampling_statergy': 'Default',\n",
              "    'min_depth': 20,\n",
              "    'max_depth': 25,\n",
              "    'min_samples_per_leaf_min_value': 5,\n",
              "    'min_samples_per_leaf_max_value': 10,\n",
              "    'parallelism': 0},\n",
              "   'GBTClassifier': {'model_name': 'Gradient Boosted Trees',\n",
              "    'is_selected': False,\n",
              "    'num_of_BoostingStages': [67, 89],\n",
              "    'feature_sampling_statergy': 'Fixed number',\n",
              "    'learningRate': [],\n",
              "    'use_deviance': True,\n",
              "    'use_exponential': False,\n",
              "    'fixed_number': 22,\n",
              "    'min_subsample': 1,\n",
              "    'max_subsample': 2,\n",
              "    'min_stepsize': 0.1,\n",
              "    'max_stepsize': 0.5,\n",
              "    'min_iter': 20,\n",
              "    'max_iter': 40,\n",
              "    'min_depth': 5,\n",
              "    'max_depth': 7},\n",
              "   'GBTRegressor': {'model_name': 'Gradient Boosted Trees',\n",
              "    'is_selected': False,\n",
              "    'num_of_BoostingStages': [67, 89],\n",
              "    'feature_sampling_statergy': 'Fixed number',\n",
              "    'use_deviance': True,\n",
              "    'use_exponential': False,\n",
              "    'fixed_number': 22,\n",
              "    'min_subsample': 1,\n",
              "    'max_subsample': 2,\n",
              "    'min_stepsize': 0.1,\n",
              "    'max_stepsize': 0.5,\n",
              "    'min_iter': 20,\n",
              "    'max_iter': 40,\n",
              "    'min_depth': 5,\n",
              "    'max_depth': 7},\n",
              "   'LinearRegression': {'model_name': 'LinearRegression',\n",
              "    'is_selected': False,\n",
              "    'parallelism': 2,\n",
              "    'min_iter': 30,\n",
              "    'max_iter': 50,\n",
              "    'min_regparam': 0.5,\n",
              "    'max_regparam': 0.8,\n",
              "    'min_elasticnet': 0.5,\n",
              "    'max_elasticnet': 0.8},\n",
              "   'LogisticRegression': {'model_name': 'LogisticRegression'},\n",
              "   'RidgeRegression': {'model_name': 'RidgeRegression',\n",
              "    'is_selected': False,\n",
              "    'regularization_term': 'Specify values to test',\n",
              "    'min_iter': 30,\n",
              "    'max_iter': 50,\n",
              "    'min_regparam': 0.5,\n",
              "    'max_regparam': 0.8},\n",
              "   'LassoRegression': {'model_name': 'Lasso Regression',\n",
              "    'is_selected': False,\n",
              "    'regularization_term': 'Specify values to test',\n",
              "    'min_iter': 30,\n",
              "    'max_iter': 50,\n",
              "    'min_regparam': 0.5,\n",
              "    'max_regparam': 0.8},\n",
              "   'ElasticNetRegression': {'model_name': 'Lasso Regression',\n",
              "    'is_selected': False,\n",
              "    'regularization_term': 'Specify values to test',\n",
              "    'min_iter': 30,\n",
              "    'max_iter': 50,\n",
              "    'min_regparam': 0.5,\n",
              "    'max_regparam': 0.8,\n",
              "    'min_elasticnet': 0.5,\n",
              "    'max_elasticnet': 0.8},\n",
              "   'xg_boost': {'model_name': 'XG Boost',\n",
              "    'is_selected': False,\n",
              "    'use_gradient_boosted_tree': True,\n",
              "    'dart': True,\n",
              "    'tree_method': '',\n",
              "    'random_state': 0,\n",
              "    'max_num_of_trees': 0,\n",
              "    'early_stopping': True,\n",
              "    'early_stopping_rounds': 2,\n",
              "    'max_depth_of_tree': [56, 89],\n",
              "    'learningRate': [89, 76],\n",
              "    'l1_regularization': [77],\n",
              "    'l2_regularization': [78],\n",
              "    'gamma': [68],\n",
              "    'min_child_weight': [67],\n",
              "    'sub_sample': [67],\n",
              "    'col_sample_by_tree': [67],\n",
              "    'replace_missing_values': False,\n",
              "    'parallelism': 0},\n",
              "   'DecisionTreeRegressor': {'model_name': 'Decision Tree',\n",
              "    'is_selected': False,\n",
              "    'min_depth': 4,\n",
              "    'max_depth': 7,\n",
              "    'use_gini': False,\n",
              "    'use_entropy': True,\n",
              "    'min_samples_per_leaf': [12, 6],\n",
              "    'use_best': True,\n",
              "    'use_random': True},\n",
              "   'DecisionTreeClassifier': {'model_name': 'Decision Tree',\n",
              "    'is_selected': False,\n",
              "    'min_depth': 4,\n",
              "    'max_depth': 7,\n",
              "    'use_gini': False,\n",
              "    'use_entropy': True,\n",
              "    'min_samples_per_leaf': [12, 6],\n",
              "    'use_best': True,\n",
              "    'use_random': True},\n",
              "   'SVM': {'model_name': 'Support Vector Machine',\n",
              "    'is_selected': False,\n",
              "    'linear_kernel': True,\n",
              "    'rep_kernel': True,\n",
              "    'polynomial_kernel': True,\n",
              "    'sigmoid_kernel': True,\n",
              "    'c_value': [566, 79],\n",
              "    'auto': True,\n",
              "    'scale': True,\n",
              "    'custom_gamma_values': True,\n",
              "    'tolerance': 7,\n",
              "    'max_iterations': 7},\n",
              "   'SGD': {'model_name': 'Stochastic Gradient Descent',\n",
              "    'is_selected': False,\n",
              "    'use_logistics': True,\n",
              "    'use_modified_hubber_loss': False,\n",
              "    'max_iterations': False,\n",
              "    'tolerance': 56,\n",
              "    'use_l1_regularization': 'on',\n",
              "    'use_l2_regularization': 'on',\n",
              "    'use_elastic_net_regularization': True,\n",
              "    'alpha_value': [79, 56],\n",
              "    'parallelism': 1},\n",
              "   'KNN': {'model_name': 'KNN',\n",
              "    'is_selected': False,\n",
              "    'k_value': [78],\n",
              "    'distance_weighting': True,\n",
              "    'neighbour_finding_algorithm': 'Automatic',\n",
              "    'random_state': 0,\n",
              "    'p_value': 0},\n",
              "   'extra_random_trees': {'model_name': 'Extra Random Trees',\n",
              "    'is_selected': False,\n",
              "    'num_of_trees': [45, 489],\n",
              "    'feature_sampling_statergy': 'Square root and Logarithm',\n",
              "    'max_depth': [12, 45],\n",
              "    'min_samples_per_leaf': [78, 56],\n",
              "    'parallelism': 3},\n",
              "   'neural_network': {'model_name': 'Neural Network',\n",
              "    'is_selected': False,\n",
              "    'hidden_layer_sizes': [67, 89],\n",
              "    'activation': '',\n",
              "    'alpha_value': 0,\n",
              "    'max_iterations': 0,\n",
              "    'convergence_tolerance': 0,\n",
              "    'early_stopping': True,\n",
              "    'solver': 'ADAM',\n",
              "    'shuffle_data': True,\n",
              "    'initial_learning_rate': 0,\n",
              "    'automatic_batching': True,\n",
              "    'beta_1': 0,\n",
              "    'beta_2': 0,\n",
              "    'epsilon': 0,\n",
              "    'power_t': 0,\n",
              "    'momentum': 0,\n",
              "    'use_nesterov_momentum': False}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGX6y-ZojtCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSNAsimLjtGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YiXJqR56jtXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('iris.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "6mIagrSB44w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "PVgEzQ5m44z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().count()"
      ],
      "metadata": {
        "id": "yyVRvFr2442d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "2BE_rN4lapYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=df['species'].unique()\n",
        "a"
      ],
      "metadata": {
        "id": "wJWIdjeq4442"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['species'].value_counts()"
      ],
      "metadata": {
        "id": "m8c_nli2cufX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(df, x= 'petal_width', y='sepal_width', hue='species')"
      ],
      "metadata": {
        "id": "hQm-WboE2_3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(df, x= 'petal_length', y='sepal_length', hue='species')"
      ],
      "metadata": {
        "id": "UzLkhm3139xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_m = df.corr()\n",
        "corr_m['sepal_width'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "erJVShzStP09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.subplots(figsize=(6, 4))\n",
        "sns.heatmap(corr_m, annot = True)\n",
        "plt.title(\"Correlations Heat Map\")"
      ],
      "metadata": {
        "id": "OGDRneJboowj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7bifQmxv7j3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = train_test_split(df, test_size=0.25)"
      ],
      "metadata": {
        "id": "E8EFsrP17kyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.shape, test_ds.shape"
      ],
      "metadata": {
        "id": "T4_rD61h7lFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_col, target_col = train_ds.columns[:-1], train_ds.columns[-1]\n",
        "input_col, target_col"
      ],
      "metadata": {
        "id": "O3X9wiRGoozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input = train_ds[input_col]\n",
        "train_tar = train_ds[target_col]\n",
        "test_input = test_ds[input_col]\n",
        "test_tar = test_ds[target_col]"
      ],
      "metadata": {
        "id": "SDmkH330_-rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input.shape, test_input.shape"
      ],
      "metadata": {
        "id": "D2QNuQgMDmTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input.head()"
      ],
      "metadata": {
        "id": "3yrkj4B5Cih2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tar.head()"
      ],
      "metadata": {
        "id": "jXocCQF9CirZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input.head()"
      ],
      "metadata": {
        "id": "xj8kBVuyCiw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tar.head()"
      ],
      "metadata": {
        "id": "BVXnA4iqCi0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "BUhqArsroo4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = XGBClassifier()\n",
        "X.fit(train_input, train_ds.species)"
      ],
      "metadata": {
        "id": "Kxj2KSBvoo8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wqmd05goo-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSdWk35mopBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9UMvIZPopD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9WAYhlVopHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZ1DndndopZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPUZxcvYopcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKDtpSDIopfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDVFrZ8J45Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzbVvUAJOBMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUkZL1whOBOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65Dq6gbUOBR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_file=json.loads(\"algoparams_from_ui.json\")\n",
        "j_file"
      ],
      "metadata": {
        "id": "GZm0cduB45Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QQaVzKvbzLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}